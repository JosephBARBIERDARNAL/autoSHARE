{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all datasets properties and store them\n",
    "\n",
    "### Code\n",
    "\n",
    "The following code **iterates over all files** in the root directory containing all SHARE datasets. It extracts the properties of each dataset and stores them in a pandas dataframe. The properties are:\n",
    "- dataset name\n",
    "- wave\n",
    "- number of rows\n",
    "- number of columns\n",
    "- column names\n",
    "\n",
    "### Details\n",
    "\n",
    "- For some reason, one or multiple files, when trying to read them, throw a `ValueError`. However, `pandas` gives us the solution by telling us we should add `convert_categoricals=False` to the `read_csv` function when this is happening. This is the reason why we have a `try` and `except` block in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:55,  5.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# initiate constants\n",
    "directory = 'data/'\n",
    "file_names = []\n",
    "waves = []\n",
    "n_rows = []\n",
    "n_columns = []\n",
    "columns = []\n",
    "\n",
    "# special case that we want to skip\n",
    "def is_special_case(file):\n",
    "    \"\"\"\n",
    "    Check if the file is a special case that we want to skip.\n",
    "    Special cases are files such as imputation files and technical variables.\n",
    "\n",
    "    Parameters:\n",
    "    file (str): The name of the file.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the file is a special case, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    special_cases = [\n",
    "        'imputation',\n",
    "        'technical_variables',\n",
    "        'dropoff',\n",
    "        'children',\n",
    "        'exrates',\n",
    "        'vignettes',\n",
    "        'weights',\n",
    "        'interviewer',\n",
    "        'ilextra',\n",
    "        'cv_r'\n",
    "    ]\n",
    "    for special_case in special_cases:\n",
    "        if special_case in file:\n",
    "            return True\n",
    "\n",
    "# iterate through all files in the directory\n",
    "for root, dirs, files in tqdm(os.walk(directory)):\n",
    "    for file in files:\n",
    "        if file.endswith('.dta'):\n",
    "\n",
    "            # skip special cases\n",
    "            if is_special_case(file):\n",
    "                # skip and continue with the next file\n",
    "                continue \n",
    "\n",
    "            try:\n",
    "                dataset = pd.read_stata(os.path.join(root, file))\n",
    "            except ValueError:\n",
    "                dataset = pd.read_stata(os.path.join(root, file), convert_categoricals=False)\n",
    "\n",
    "            # get all meta data from current file\n",
    "            file_names.append(file)\n",
    "            waves.append(file[6])\n",
    "            n_rows.append(len(dataset))\n",
    "            n_columns.append(len(dataset.columns))\n",
    "            columns.append(list(dataset.columns))\n",
    "\n",
    "# create a dataframe with the results\n",
    "df = pd.DataFrame({\n",
    "    'file_name': file_names,\n",
    "    'wave': waves,\n",
    "    'n_rows': n_rows,\n",
    "    'n_columns': n_columns,\n",
    "    'columns': columns\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminate normal and particular datasets\n",
    "\n",
    "According to the [official documentation](https://share-eric.eu/data/faqs-support):\n",
    "\n",
    "*\"The naming of variables is harmonised across waves. Variable names in the CAPI instrument data use the following format: mmXXXyyy_LL. “mm” is the module identifier, e.g. DN for the demographics module, “XXX” refers to the question number, e.g. 001, and “yyy” are optional digits for dummy variables (indicated by “d”), euro conversion (indicated by “e”) or unfolding brackets (indicated by “ub”). The separation character “_” is followed by “LL” optional digits for category or loop indication (“outer loop”).\"*\n",
    "\n",
    "For this reason, we add **boolean indicator** columns to the dataframe to discriminate normal datasets from particular ones. This allows us to see that approximately 10% of the datasets are particular, and maybe useless for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>wave</th>\n",
       "      <th>n_rows</th>\n",
       "      <th>n_columns</th>\n",
       "      <th>columns</th>\n",
       "      <th>is_normal</th>\n",
       "      <th>is_gv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>sharew3_rel9-0-0_xt.dta</td>\n",
       "      <td>3</td>\n",
       "      <td>1207</td>\n",
       "      <td>172</td>\n",
       "      <td>[mergeid, hhid3, country, language_xt, gender_...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>sharew1_rel9-0-0_cf.dta</td>\n",
       "      <td>1</td>\n",
       "      <td>30416</td>\n",
       "      <td>24</td>\n",
       "      <td>[mergeid, hhid1, mergeidp1, coupleid1, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>sharew7_rel9-0-0_as.dta</td>\n",
       "      <td>7</td>\n",
       "      <td>77181</td>\n",
       "      <td>95</td>\n",
       "      <td>[mergeid, hhid7, mergeidp7, coupleid7, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sharew5_rel9-0-0_gs.dta</td>\n",
       "      <td>5</td>\n",
       "      <td>66038</td>\n",
       "      <td>23</td>\n",
       "      <td>[mergeid, hhid5, mergeidp5, coupleid5, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>sharew7_rel9-0-0_gs.dta</td>\n",
       "      <td>7</td>\n",
       "      <td>77181</td>\n",
       "      <td>23</td>\n",
       "      <td>[mergeid, hhid7, mergeidp7, coupleid7, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>sharew6_rel9-0-0_pf.dta</td>\n",
       "      <td>6</td>\n",
       "      <td>68055</td>\n",
       "      <td>18</td>\n",
       "      <td>[mergeid, hhid6, mergeidp6, coupleid6, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>sharew6_rel9-0-0_gv_networks.dta</td>\n",
       "      <td>6</td>\n",
       "      <td>68055</td>\n",
       "      <td>231</td>\n",
       "      <td>[mergeid, hhid6, mergeidp6, coupleid6, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>sharew2_rel9-0-0_ch.dta</td>\n",
       "      <td>2</td>\n",
       "      <td>37132</td>\n",
       "      <td>223</td>\n",
       "      <td>[mergeid, hhid2, mergeidp2, coupleid2, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>sharew7_rel9-0-0_gv_big5.dta</td>\n",
       "      <td>7</td>\n",
       "      <td>77181</td>\n",
       "      <td>11</td>\n",
       "      <td>[mergeid, hhid7, mergeidp7, coupleid7, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>sharew8_rel9-0-0_gv_accelerometer_sleep.dta</td>\n",
       "      <td>8</td>\n",
       "      <td>6254</td>\n",
       "      <td>11</td>\n",
       "      <td>[mergeid, country, language, ActiPASS_LongBedt...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       file_name wave  n_rows  n_columns  \\\n",
       "203                      sharew3_rel9-0-0_xt.dta    3    1207        172   \n",
       "55                       sharew1_rel9-0-0_cf.dta    1   30416         24   \n",
       "130                      sharew7_rel9-0-0_as.dta    7   77181         95   \n",
       "9                        sharew5_rel9-0-0_gs.dta    5   66038         23   \n",
       "121                      sharew7_rel9-0-0_gs.dta    7   77181         23   \n",
       "173                      sharew6_rel9-0-0_pf.dta    6   68055         18   \n",
       "185             sharew6_rel9-0-0_gv_networks.dta    6   68055        231   \n",
       "76                       sharew2_rel9-0-0_ch.dta    2   37132        223   \n",
       "133                 sharew7_rel9-0-0_gv_big5.dta    7   77181         11   \n",
       "138  sharew8_rel9-0-0_gv_accelerometer_sleep.dta    8    6254         11   \n",
       "\n",
       "                                               columns  is_normal  is_gv  \n",
       "203  [mergeid, hhid3, country, language_xt, gender_...       True  False  \n",
       "55   [mergeid, hhid1, mergeidp1, coupleid1, country...       True  False  \n",
       "130  [mergeid, hhid7, mergeidp7, coupleid7, country...       True  False  \n",
       "9    [mergeid, hhid5, mergeidp5, coupleid5, country...       True  False  \n",
       "121  [mergeid, hhid7, mergeidp7, coupleid7, country...       True  False  \n",
       "173  [mergeid, hhid6, mergeidp6, coupleid6, country...       True  False  \n",
       "185  [mergeid, hhid6, mergeidp6, coupleid6, country...       True   True  \n",
       "76   [mergeid, hhid2, mergeidp2, coupleid2, country...       True  False  \n",
       "133  [mergeid, hhid7, mergeidp7, coupleid7, country...       True   True  \n",
       "138  [mergeid, country, language, ActiPASS_LongBedt...       True   True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_file_normal(file_name: str, only_gv: bool=True) -> bool:\n",
    "    \"\"\"\n",
    "    Detect if the last 3 elements of the string follow the pattern _ab, \n",
    "    where 'a' and 'b' are single letters.\n",
    "    \n",
    "    Args:\n",
    "    - file_name (str): The string to be checked.\n",
    "    Returns:\n",
    "    - bool: True if the pattern is found, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # special case the General Variables files\n",
    "    if '_gv_' in file_name:\n",
    "        return True\n",
    "    \n",
    "    if only_gv:\n",
    "        return False\n",
    "    \n",
    "    # get file extension\n",
    "    suffix = file_name[:-4]\n",
    "\n",
    "    # check if the suffix is long enough\n",
    "    if len(suffix) < 3:\n",
    "        return False\n",
    "    \n",
    "    # check if the last three elements follow the pattern\n",
    "    last_three = suffix[-3:]\n",
    "    if last_three[0] == '_' and last_three[1].isalpha() and last_three[2].isalpha():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# filter out the files that are not normal\n",
    "df['is_normal'] = df['file_name'].apply(is_file_normal, only_gv=False)\n",
    "df['is_gv'] = df['file_name'].apply(is_file_normal, only_gv=True)\n",
    "\n",
    "# save the dataframe to a csv file\n",
    "df.to_csv('data_info.csv', index=False)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_normal\n",
      "True    100.0\n",
      "Name: count, dtype: float64 \n",
      "\n",
      "is_gv\n",
      "False    83.26\n",
      "True     16.74\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(round(df.is_normal.value_counts()/len(df)*100,2), '\\n')\n",
    "print(round(df.is_gv.value_counts()/len(df)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get column names for each wave\n",
    "\n",
    "We store the column names for each wave in `.csv` files, so we can use them later. The aim is that we don't have to open all files just for the columns names.\n",
    "\n",
    "For the variable selection feature, we only want to **keep relevant variables**. For this, we filter on variables that are not identifiers (such as `mergeid`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns_to_remove = ['mergeid', 'hhid', 'coupleid'] \n",
    "\n",
    "def is_valid_column(col_name, patterns_to_remove=patterns_to_remove):\n",
    "    \"\"\"\n",
    "    Check if a column name is not an identifier.\n",
    "    \n",
    "    Example: if a column is named 'mergeid6', it respects the\n",
    "    pattern 'mergeid' and should be removed.\n",
    "    \n",
    "    Args:\n",
    "    - col_name (str): The column name to be checked.\n",
    "    - patterns_to_remove (list): A list of patterns to be removed.\n",
    "    Returns:\n",
    "    - bool: True if the column name is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    for pattern in patterns_to_remove:\n",
    "        if pattern in col_name:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave 1 has 717 unique columns.\n",
      "Wave 2 has 275 unique columns.\n",
      "Wave 3 has 2587 unique columns.\n",
      "Wave 4 has 318 unique columns.\n",
      "Wave 5 has 99 unique columns.\n",
      "Wave 6 has 247 unique columns.\n",
      "Wave 7 has 2534 unique columns.\n",
      "Wave 8 has 507 unique columns.\n",
      "Wave 9 has 757 unique columns.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data_info.csv')\n",
    "\n",
    "# iterate through the waves\n",
    "for wave in range(1,9+1):\n",
    "    columns_properties = pd.DataFrame()\n",
    "    subset = df[df['wave'] == wave]\n",
    "\n",
    "    # iterate through the files in the wave\n",
    "    for columns,filename in zip(subset['columns'],subset['file_name']):\n",
    "        \n",
    "        # get all columns and the file they belong to\n",
    "        row = list(columns[1:-1].replace(\"'\", \"\").split(', '))\n",
    "        filenames = [filename]*len(row)\n",
    "\n",
    "        # add the columns and the file to the dataframe\n",
    "        temp = pd.DataFrame({'column': row, 'file_name': filenames})\n",
    "        columns_properties = pd.concat([columns_properties, temp])\n",
    "\n",
    "    columns_properties.drop_duplicates(inplace=True)\n",
    "    print(f'Wave {wave} has {len(columns)} unique columns.')\n",
    "\n",
    "    # remove all column names dupplicate (keep first occurence)\n",
    "    columns_properties.drop_duplicates(subset='column', keep='first', inplace=True)\n",
    "\n",
    "    # remove all columns that are identifiers\n",
    "    columns_properties['is_valid'] = columns_properties['column'].apply(is_valid_column)\n",
    "    columns_properties = columns_properties[columns_properties['is_valid'] == True]\n",
    "\n",
    "    # save as csv file\n",
    "    columns_properties.to_csv(f'columns/wave_{wave}_columns.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create basic dataset per wave\n",
    "\n",
    "The aim of this code is to create a basic/dummy dataset per wave. This dataset contains the following columns:\n",
    "\n",
    "- `mergeid`: identifier of the respondent\n",
    "- `country`: country of the respondent\n",
    "- `language`: language of the respondent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  4.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for wave in tqdm(range(1,9+1)):\n",
    "    df = pd.read_stata(f'data/sharew{wave}_rel9-0-0_ALL_datasets_stata/sharew{wave}_rel9-0-0_ac.dta')\n",
    "    df = df[['country', 'mergeid', 'language']]\n",
    "    df.to_stata(f'data/sharew{wave}_rel9-0-0_ALL_datasets_stata/wave{wave}_dummy.stata', write_index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
