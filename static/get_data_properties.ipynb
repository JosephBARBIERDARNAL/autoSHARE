{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all datasets properties and store them\n",
    "\n",
    "### Code\n",
    "\n",
    "The following code **iterates over all files** in the root directory containing all SHARE datasets. It extracts the properties of each dataset and stores them in a pandas dataframe. The properties are:\n",
    "- dataset name\n",
    "- number of rows\n",
    "- number of columns\n",
    "- column names\n",
    "\n",
    "### Details\n",
    "\n",
    "- For some reason, one or multiple files, when trying to read them, throw a `ValueError`. However, `pandas` gives us the solution by telling us we should add `convert_categoricals=False` to the `read_csv` function when this is happening. This is the reason why we have a `try` and `except` block in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [01:20,  8.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# initiate constants\n",
    "directory = 'data/'\n",
    "file_names = []\n",
    "waves = []\n",
    "n_rows = []\n",
    "n_columns = []\n",
    "columns = []\n",
    "\n",
    "# iterate through all files in the directory\n",
    "for root, dirs, files in tqdm(os.walk(directory)):\n",
    "    for file in files:\n",
    "        if file.endswith('.dta'):\n",
    "            try:\n",
    "                dataset = pd.read_stata(os.path.join(root, file))\n",
    "            except ValueError:\n",
    "                dataset = pd.read_stata(os.path.join(root, file), convert_categoricals=False)\n",
    "            file_names.append(file)\n",
    "            waves.append(file[6])\n",
    "            n_rows.append(len(dataset))\n",
    "            n_columns.append(len(dataset.columns))\n",
    "            columns.append(list(dataset.columns))\n",
    "\n",
    "# create a dataframe with the results\n",
    "df = pd.DataFrame({\n",
    "    'file_name': file_names,\n",
    "    'wave': waves,\n",
    "    'n_rows': n_rows,\n",
    "    'n_columns': n_columns,\n",
    "    'columns': columns\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminate normal and particular datasets\n",
    "\n",
    "According to the [official documentation](https://share-eric.eu/data/faqs-support):\n",
    "\n",
    "*\"The naming of variables is harmonised across waves. Variable names in the CAPI instrument data use the following format: mmXXXyyy_LL. “mm” is the module identifier, e.g. DN for the demographics module, “XXX” refers to the question number, e.g. 001, and “yyy” are optional digits for dummy variables (indicated by “d”), euro conversion (indicated by “e”) or unfolding brackets (indicated by “ub”). The separation character “_” is followed by “LL” optional digits for category or loop indication (“outer loop”).\"*\n",
    "\n",
    "For this reason, we add **boolean indicator** columns to the dataframe to discriminate normal datasets from particular ones. This allows us to see that approximately 10% of the datasets are particular, and maybe useless for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_normal(file_name: str, only_gv: bool=True) -> bool:\n",
    "    \"\"\"\n",
    "    Detect if the last 3 elements of the string follow the pattern _ab, \n",
    "    where 'a' and 'b' are single letters.\n",
    "    \n",
    "    Args:\n",
    "    - file_name (str): The string to be checked.\n",
    "    Returns:\n",
    "    - bool: True if the pattern is found, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # special case the General Variables files\n",
    "    if '_gv_' in file_name:\n",
    "        return True\n",
    "    \n",
    "    if only_gv:\n",
    "        return False\n",
    "    \n",
    "    # remove file extension\n",
    "    suffix = file_name[:-4]\n",
    "\n",
    "    # check if the suffix is long enough\n",
    "    if len(suffix) < 3:\n",
    "        return False\n",
    "    \n",
    "    # check if the last three elements follow the pattern\n",
    "    last_three = suffix[-3:]\n",
    "    if last_three[0] == '_' and last_three[1].isalpha() and last_three[2].isalpha():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# filter out the files that are not normal\n",
    "df['is_normal'] = df['file_name'].apply(is_file_normal, only_gv=False)\n",
    "df['is_gv'] = df['file_name'].apply(is_file_normal, only_gv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_normal\n",
      "True     89.43\n",
      "False    10.57\n",
      "Name: count, dtype: float64\n",
      "\n",
      "is_gv\n",
      "False    77.36\n",
      "True     22.64\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(round(df.is_normal.value_counts()/len(df)*100,2))\n",
    "print()\n",
    "print(round(df.is_gv.value_counts()/len(df)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>wave</th>\n",
       "      <th>n_rows</th>\n",
       "      <th>n_columns</th>\n",
       "      <th>columns</th>\n",
       "      <th>is_normal</th>\n",
       "      <th>is_gv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>sharew6_rel8-0-0_pf.dta</td>\n",
       "      <td>6</td>\n",
       "      <td>68085</td>\n",
       "      <td>18</td>\n",
       "      <td>[mergeid, hhid6, mergeidp6, coupleid6, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>sharew5_rel8-0-0_ch.dta</td>\n",
       "      <td>5</td>\n",
       "      <td>66065</td>\n",
       "      <td>1375</td>\n",
       "      <td>[mergeid, hhid5, mergeidp5, coupleid5, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>sharew4_rel8-0-0_cf.dta</td>\n",
       "      <td>4</td>\n",
       "      <td>58000</td>\n",
       "      <td>36</td>\n",
       "      <td>[mergeid, hhid4, mergeidp4, coupleid4, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sharew4_rel8-0-0_dropoff.dta</td>\n",
       "      <td>4</td>\n",
       "      <td>46696</td>\n",
       "      <td>1261</td>\n",
       "      <td>[mergeid, hhid4, mergeidp4, coupleid4, country...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>sharew8_rel8-0-0_ex.dta</td>\n",
       "      <td>8</td>\n",
       "      <td>46733</td>\n",
       "      <td>26</td>\n",
       "      <td>[mergeid, hhid8, mergeidp8, coupleid8, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>sharew2_rel8-0-0_ph.dta</td>\n",
       "      <td>2</td>\n",
       "      <td>37143</td>\n",
       "      <td>168</td>\n",
       "      <td>[mergeid, hhid2, mergeidp2, coupleid2, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>sharew6_rel8-0-0_co.dta</td>\n",
       "      <td>6</td>\n",
       "      <td>68085</td>\n",
       "      <td>29</td>\n",
       "      <td>[mergeid, hhid6, mergeidp6, coupleid6, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>sharew4_rel8-0-0_gv_weights.dta</td>\n",
       "      <td>4</td>\n",
       "      <td>58000</td>\n",
       "      <td>14</td>\n",
       "      <td>[mergeid, hhid4, mergeidp4, coupleid4, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sharew1_rel8-0-0_hc.dta</td>\n",
       "      <td>1</td>\n",
       "      <td>30419</td>\n",
       "      <td>157</td>\n",
       "      <td>[mergeid, hhid1, mergeidp1, coupleid1, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>sharew3_rel8-0-0_xt.dta</td>\n",
       "      <td>3</td>\n",
       "      <td>1207</td>\n",
       "      <td>172</td>\n",
       "      <td>[mergeid, hhid3, country, language_xt, gender_...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           file_name wave  n_rows  n_columns  \\\n",
       "136          sharew6_rel8-0-0_pf.dta    6   68085         18   \n",
       "81           sharew5_rel8-0-0_ch.dta    5   66065       1375   \n",
       "32           sharew4_rel8-0-0_cf.dta    4   58000         36   \n",
       "33      sharew4_rel8-0-0_dropoff.dta    4   46696       1261   \n",
       "162          sharew8_rel8-0-0_ex.dta    8   46733         26   \n",
       "264          sharew2_rel8-0-0_ph.dta    2   37143        168   \n",
       "116          sharew6_rel8-0-0_co.dta    6   68085         29   \n",
       "42   sharew4_rel8-0-0_gv_weights.dta    4   58000         14   \n",
       "21           sharew1_rel8-0-0_hc.dta    1   30419        157   \n",
       "112          sharew3_rel8-0-0_xt.dta    3    1207        172   \n",
       "\n",
       "                                               columns  is_normal  is_gv  \n",
       "136  [mergeid, hhid6, mergeidp6, coupleid6, country...       True  False  \n",
       "81   [mergeid, hhid5, mergeidp5, coupleid5, country...       True  False  \n",
       "32   [mergeid, hhid4, mergeidp4, coupleid4, country...       True  False  \n",
       "33   [mergeid, hhid4, mergeidp4, coupleid4, country...      False  False  \n",
       "162  [mergeid, hhid8, mergeidp8, coupleid8, country...       True  False  \n",
       "264  [mergeid, hhid2, mergeidp2, coupleid2, country...       True  False  \n",
       "116  [mergeid, hhid6, mergeidp6, coupleid6, country...       True  False  \n",
       "42   [mergeid, hhid4, mergeidp4, coupleid4, country...       True   True  \n",
       "21   [mergeid, hhid1, mergeidp1, coupleid1, country...       True  False  \n",
       "112  [mergeid, hhid3, country, language_xt, gender_...       True  False  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the dataframe to a csv file\n",
    "df.to_csv('data_info.csv', index=False)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get column names for each wave\n",
    "\n",
    "We store the column names for each wave in `.txt` files, so we can use them later. The aim is that we don't have to open all files just for the columns names.\n",
    "\n",
    "Also, the following code filter on datasets so that it only uses the **generated variables** datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave 1 has 371 unique columns.\n",
      "Wave 2 has 341 unique columns.\n",
      "Wave 3 has 82 unique columns.\n",
      "Wave 4 has 454 unique columns.\n",
      "Wave 5 has 422 unique columns.\n",
      "Wave 6 has 1500 unique columns.\n",
      "Wave 7 has 1284 unique columns.\n",
      "Wave 8 has 2289 unique columns.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data_info.csv')\n",
    "df = df[df['is_gv'] == True] # only keep the general variables files\n",
    "\n",
    "# iterate through the waves\n",
    "for wave in range(1,9):\n",
    "    columns = []\n",
    "    subset = df[df['wave'] == wave]\n",
    "    for row in subset['columns']:\n",
    "        row = list(row[1:-1].replace(\"'\", \"\").split(', '))\n",
    "        columns.extend(row)\n",
    "    columns = list(set(columns))\n",
    "    columns.sort()\n",
    "    print(f'Wave {wave} has {len(columns)} unique columns.')\n",
    "\n",
    "    # save as txt file\n",
    "    with open(f'columns/wave_{wave}_columns.txt', 'w') as f:\n",
    "        for item in columns:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
