{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all datasets properties and store them\n",
    "\n",
    "### Code\n",
    "\n",
    "The following code **iterates over all files** in the root directory containing all SHARE datasets. It extracts the properties of each dataset and stores them in a pandas dataframe. The properties are:\n",
    "- dataset name\n",
    "- number of rows\n",
    "- number of columns\n",
    "- column names\n",
    "\n",
    "### Details\n",
    "\n",
    "- For some reason, one or multiple files, when trying to read them, throw a `ValueError`. However, `pandas` gives us the solution by telling us we should add `convert_categoricals=False` to the `read_csv` function when this is happening. This is the reason why we have a `try` and `except` block in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [01:21,  9.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# initiate constants\n",
    "directory = '../../SHARE/data/'\n",
    "file_names = []\n",
    "waves = []\n",
    "n_rows = []\n",
    "n_columns = []\n",
    "columns = []\n",
    "\n",
    "# iterate through all files in the directory\n",
    "for root, dirs, files in tqdm(os.walk(directory)):\n",
    "    for file in files:\n",
    "        if file.endswith('.dta'):\n",
    "            try:\n",
    "                dataset = pd.read_stata(os.path.join(root, file))\n",
    "            except ValueError:\n",
    "                dataset = pd.read_stata(os.path.join(root, file), convert_categoricals=False)\n",
    "            file_names.append(file)\n",
    "            waves.append(file[6])\n",
    "            n_rows.append(len(dataset))\n",
    "            n_columns.append(len(dataset.columns))\n",
    "            columns.append(list(dataset.columns))\n",
    "\n",
    "# create a dataframe with the results\n",
    "df = pd.DataFrame({\n",
    "    'file_name': file_names,\n",
    "    'wave': waves,\n",
    "    'n_rows': n_rows,\n",
    "    'n_columns': n_columns,\n",
    "    'columns': columns\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminate normal and particular datasets\n",
    "\n",
    "According to the [official documentation](https://share-eric.eu/data/faqs-support):\n",
    "\n",
    "*\"The naming of variables is harmonised across waves. Variable names in the CAPI instrument data use the following format: mmXXXyyy_LL. “mm” is the module identifier, e.g. DN for the demographics module, “XXX” refers to the question number, e.g. 001, and “yyy” are optional digits for dummy variables (indicated by “d”), euro conversion (indicated by “e”) or unfolding brackets (indicated by “ub”). The separation character “_” is followed by “LL” optional digits for category or loop indication (“outer loop”).\"*\n",
    "\n",
    "For this reason, we add **boolean indicator** columns to the dataframe to discriminate normal datasets from particular ones. This allows us to see that approximately 10% of the datasets are particular, and maybe useless for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_normal\n",
      "True     89.43\n",
      "False    10.57\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def is_file_normal(file_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect if the last 3 elements of the string follow the pattern _ab, \n",
    "    where 'a' and 'b' are single letters.\n",
    "    \n",
    "    Args:\n",
    "    - file_name (str): The string to be checked.\n",
    "    Returns:\n",
    "    - bool: True if the pattern is found, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # special case the General Variables files\n",
    "    if '_gv_' in file_name:\n",
    "        return True\n",
    "    \n",
    "    # remove file extension\n",
    "    suffix = file_name[:-4]\n",
    "\n",
    "    # check if the suffix is long enough\n",
    "    if len(suffix) < 3:\n",
    "        return False\n",
    "    \n",
    "    # check if the last three elements follow the pattern\n",
    "    last_three = suffix[-3:]\n",
    "    if last_three[0] == '_' and last_three[1].isalpha() and last_three[2].isalpha():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# filter out the files that are not normal\n",
    "df['is_normal'] = df['file_name'].apply(is_file_normal)\n",
    "print(round(df.is_normal.value_counts()/len(df)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>wave</th>\n",
       "      <th>n_rows</th>\n",
       "      <th>n_columns</th>\n",
       "      <th>columns</th>\n",
       "      <th>is_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>sharew2_rel8-0-0_br.dta</td>\n",
       "      <td>2</td>\n",
       "      <td>37143</td>\n",
       "      <td>23</td>\n",
       "      <td>[mergeid, hhid2, mergeidp2, coupleid2, country...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>sharew3_rel8-0-0_gv_weights.dta</td>\n",
       "      <td>3</td>\n",
       "      <td>28463</td>\n",
       "      <td>14</td>\n",
       "      <td>[mergeid, hhid3, mergeidp3, coupleid3, country...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>sharew4_rel8-0-0_co.dta</td>\n",
       "      <td>4</td>\n",
       "      <td>58000</td>\n",
       "      <td>25</td>\n",
       "      <td>[mergeid, hhid4, mergeidp4, coupleid4, country...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>sharew2_rel8-0-0_gv_imputations.dta</td>\n",
       "      <td>2</td>\n",
       "      <td>185715</td>\n",
       "      <td>245</td>\n",
       "      <td>[mergeid, hhid2, mergeidp2, coupleid2, country...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>sharew4_rel8-0-0_gv_exrates.dta</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>69</td>\n",
       "      <td>[country, euro, currency, exrate_w1, exrate_w2...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sharew1_rel8-0-0_gv_weights.dta</td>\n",
       "      <td>1</td>\n",
       "      <td>30419</td>\n",
       "      <td>14</td>\n",
       "      <td>[mergeid, hhid1, mergeidp1, coupleid1, country...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>sharew8_rel8-0-0_cf.dta</td>\n",
       "      <td>8</td>\n",
       "      <td>46733</td>\n",
       "      <td>75</td>\n",
       "      <td>[mergeid, hhid8, mergeidp8, coupleid8, country...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>sharew1_rel8-0-0_ep_ilextra.dta</td>\n",
       "      <td>1</td>\n",
       "      <td>575</td>\n",
       "      <td>25</td>\n",
       "      <td>[mergeid, hhid1, mergeidp1, coupleid1, country...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>sharew8_rel8-0-0_gv_accelerometer_hour.dta</td>\n",
       "      <td>8</td>\n",
       "      <td>153600</td>\n",
       "      <td>256</td>\n",
       "      <td>[mergeid, country, language, month, year, week...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>sharew4_rel8-0-0_mh.dta</td>\n",
       "      <td>4</td>\n",
       "      <td>58000</td>\n",
       "      <td>35</td>\n",
       "      <td>[mergeid, hhid4, mergeidp4, coupleid4, country...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      file_name wave  n_rows  n_columns  \\\n",
       "244                     sharew2_rel8-0-0_br.dta    2   37143         23   \n",
       "99              sharew3_rel8-0-0_gv_weights.dta    3   28463         14   \n",
       "59                      sharew4_rel8-0-0_co.dta    4   58000         25   \n",
       "254         sharew2_rel8-0-0_gv_imputations.dta    2  185715        245   \n",
       "55              sharew4_rel8-0-0_gv_exrates.dta    4      29         69   \n",
       "31              sharew1_rel8-0-0_gv_weights.dta    1   30419         14   \n",
       "175                     sharew8_rel8-0-0_cf.dta    8   46733         75   \n",
       "30              sharew1_rel8-0-0_ep_ilextra.dta    1     575         25   \n",
       "183  sharew8_rel8-0-0_gv_accelerometer_hour.dta    8  153600        256   \n",
       "34                      sharew4_rel8-0-0_mh.dta    4   58000         35   \n",
       "\n",
       "                                               columns  is_normal  \n",
       "244  [mergeid, hhid2, mergeidp2, coupleid2, country...       True  \n",
       "99   [mergeid, hhid3, mergeidp3, coupleid3, country...       True  \n",
       "59   [mergeid, hhid4, mergeidp4, coupleid4, country...       True  \n",
       "254  [mergeid, hhid2, mergeidp2, coupleid2, country...       True  \n",
       "55   [country, euro, currency, exrate_w1, exrate_w2...       True  \n",
       "31   [mergeid, hhid1, mergeidp1, coupleid1, country...       True  \n",
       "175  [mergeid, hhid8, mergeidp8, coupleid8, country...       True  \n",
       "30   [mergeid, hhid1, mergeidp1, coupleid1, country...      False  \n",
       "183  [mergeid, country, language, month, year, week...       True  \n",
       "34   [mergeid, hhid4, mergeidp4, coupleid4, country...       True  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the dataframe to a csv file\n",
    "df.to_csv('data_info.csv', index=False)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get column names for each wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n",
      "265\n",
      "265\n",
      "265\n",
      "265\n",
      "265\n",
      "265\n",
      "265\n"
     ]
    }
   ],
   "source": [
    "for wave in range(1,9):\n",
    "    print(len(df['wave'] == str(wave)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave 1 has 2201 unique columns.\n",
      "Wave 2 has 2496 unique columns.\n",
      "Wave 3 has 3264 unique columns.\n",
      "Wave 4 has 4203 unique columns.\n",
      "Wave 5 has 3816 unique columns.\n",
      "Wave 6 has 5406 unique columns.\n",
      "Wave 7 has 8014 unique columns.\n",
      "Wave 8 has 6478 unique columns.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data_info.csv')\n",
    "\n",
    "# iterate through the waves\n",
    "for wave in range(1,9):\n",
    "    columns = []\n",
    "    subset = df[df['wave'] == wave]\n",
    "    for row in subset['columns']:\n",
    "        row = row[1:-1].replace(\"'\", \"\").split(', ')\n",
    "        columns.extend(list(row))\n",
    "    columns = list(set(columns))\n",
    "    print(f'Wave {wave} has {len(columns)} unique columns.')\n",
    "\n",
    "    # save as txt file\n",
    "    with open(f'columns/wave_{wave}_columns.txt', 'w') as f:\n",
    "        for item in columns:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
