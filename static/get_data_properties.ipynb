{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all datasets properties and store them\n",
    "\n",
    "### Code\n",
    "\n",
    "The following code **iterates over all files** in the root directory containing all SHARE datasets. It extracts the properties of each dataset and stores them in a pandas dataframe. The properties are:\n",
    "- dataset name\n",
    "- wave\n",
    "- number of rows\n",
    "- number of columns\n",
    "- column names\n",
    "\n",
    "### Details\n",
    "\n",
    "- For some reason, one or multiple files, when trying to read them, throw a `ValueError`. However, `pandas` gives us the solution by telling us we should add `convert_categoricals=False` to the `read_csv` function when this is happening. This is the reason why we have a `try` and `except` block in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:53, 11.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# initiate constants\n",
    "directory = 'data/'\n",
    "file_names = []\n",
    "waves = []\n",
    "n_rows = []\n",
    "n_columns = []\n",
    "columns = []\n",
    "\n",
    "# iterate through all files in the directory\n",
    "for root, dirs, files in tqdm(os.walk(directory)):\n",
    "    for file in files:\n",
    "        if file.endswith('.dta'):\n",
    "            try:\n",
    "                dataset = pd.read_stata(os.path.join(root, file))\n",
    "            except ValueError:\n",
    "                dataset = pd.read_stata(os.path.join(root, file), convert_categoricals=False)\n",
    "            file_names.append(file)\n",
    "            waves.append(file[6])\n",
    "            n_rows.append(len(dataset))\n",
    "            n_columns.append(len(dataset.columns))\n",
    "            columns.append(list(dataset.columns))\n",
    "\n",
    "# create a dataframe with the results\n",
    "df = pd.DataFrame({\n",
    "    'file_name': file_names,\n",
    "    'wave': waves,\n",
    "    'n_rows': n_rows,\n",
    "    'n_columns': n_columns,\n",
    "    'columns': columns\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminate normal and particular datasets\n",
    "\n",
    "According to the [official documentation](https://share-eric.eu/data/faqs-support):\n",
    "\n",
    "*\"The naming of variables is harmonised across waves. Variable names in the CAPI instrument data use the following format: mmXXXyyy_LL. “mm” is the module identifier, e.g. DN for the demographics module, “XXX” refers to the question number, e.g. 001, and “yyy” are optional digits for dummy variables (indicated by “d”), euro conversion (indicated by “e”) or unfolding brackets (indicated by “ub”). The separation character “_” is followed by “LL” optional digits for category or loop indication (“outer loop”).\"*\n",
    "\n",
    "For this reason, we add **boolean indicator** columns to the dataframe to discriminate normal datasets from particular ones. This allows us to see that approximately 10% of the datasets are particular, and maybe useless for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_file_normal(file_name: str, only_gv: bool=True) -> bool:\n",
    "    \"\"\"\n",
    "    Detect if the last 3 elements of the string follow the pattern _ab, \n",
    "    where 'a' and 'b' are single letters.\n",
    "    \n",
    "    Args:\n",
    "    - file_name (str): The string to be checked.\n",
    "    Returns:\n",
    "    - bool: True if the pattern is found, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # special case the General Variables files\n",
    "    if '_gv_' in file_name:\n",
    "        return True\n",
    "    \n",
    "    if only_gv:\n",
    "        return False\n",
    "    \n",
    "    # remove file extension\n",
    "    suffix = file_name[:-4]\n",
    "\n",
    "    # check if the suffix is long enough\n",
    "    if len(suffix) < 3:\n",
    "        return False\n",
    "    \n",
    "    # check if the last three elements follow the pattern\n",
    "    last_three = suffix[-3:]\n",
    "    if last_three[0] == '_' and last_three[1].isalpha() and last_three[2].isalpha():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# filter out the files that are not normal\n",
    "df['is_normal'] = df['file_name'].apply(is_file_normal, only_gv=False)\n",
    "df['is_gv'] = df['file_name'].apply(is_file_normal, only_gv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_normal\n",
      "True     89.44\n",
      "False    10.56\n",
      "Name: count, dtype: float64\n",
      "\n",
      "is_gv\n",
      "False    76.24\n",
      "True     23.76\n",
      "Name: count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(round(df.is_normal.value_counts()/len(df)*100,2))\n",
    "print()\n",
    "print(round(df.is_gv.value_counts()/len(df)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>wave</th>\n",
       "      <th>n_rows</th>\n",
       "      <th>n_columns</th>\n",
       "      <th>columns</th>\n",
       "      <th>is_normal</th>\n",
       "      <th>is_gv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>sharew8_rel9-0-0_xt.dta</td>\n",
       "      <td>8</td>\n",
       "      <td>4175</td>\n",
       "      <td>202</td>\n",
       "      <td>[mergeid, hhid8, country, language_xt, gender_...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>sharew2_rel9-0-0_technical_variables.dta</td>\n",
       "      <td>2</td>\n",
       "      <td>37132</td>\n",
       "      <td>16</td>\n",
       "      <td>[mergeid, hhid2, mergeidp2, coupleid2, country...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>sharew9_rel9-0-0_xt.dta</td>\n",
       "      <td>9</td>\n",
       "      <td>3491</td>\n",
       "      <td>220</td>\n",
       "      <td>[mergeid, hhid9, country, language_xt, gender_...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>sharew3_rel9-0-0_gv_weights.dta</td>\n",
       "      <td>3</td>\n",
       "      <td>28454</td>\n",
       "      <td>14</td>\n",
       "      <td>[mergeid, hhid3, mergeidp3, coupleid3, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>sharew2_rel9-0-0_gv_health.dta</td>\n",
       "      <td>2</td>\n",
       "      <td>37132</td>\n",
       "      <td>44</td>\n",
       "      <td>[mergeid, hhid2, mergeidp2, coupleid2, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>sharew8_rel9-0-0_as.dta</td>\n",
       "      <td>8</td>\n",
       "      <td>53695</td>\n",
       "      <td>95</td>\n",
       "      <td>[mergeid, hhid8, mergeidp8, coupleid8, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>sharew3_rel9-0-0_dq.dta</td>\n",
       "      <td>3</td>\n",
       "      <td>28454</td>\n",
       "      <td>177</td>\n",
       "      <td>[mergeid, hhid3, mergeidp3, coupleid3, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sharew5_rel9-0-0_gv_imputations.dta</td>\n",
       "      <td>5</td>\n",
       "      <td>330190</td>\n",
       "      <td>279</td>\n",
       "      <td>[mergeid, hhid5, mergeidp5, coupleid5, country...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>sharew2_rel9-0-0_cv_r.dta</td>\n",
       "      <td>2</td>\n",
       "      <td>55262</td>\n",
       "      <td>30</td>\n",
       "      <td>[mergeid, hhid2, mergeidp2, coupleid2, country...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>sharew2_rel9-0-0_xt.dta</td>\n",
       "      <td>2</td>\n",
       "      <td>726</td>\n",
       "      <td>108</td>\n",
       "      <td>[mergeid, hhid2, country, language_xt, gender_...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    file_name wave  n_rows  n_columns  \\\n",
       "209                   sharew8_rel9-0-0_xt.dta    8    4175        202   \n",
       "128  sharew2_rel9-0-0_technical_variables.dta    2   37132         16   \n",
       "278                   sharew9_rel9-0-0_xt.dta    9    3491        220   \n",
       "264           sharew3_rel9-0-0_gv_weights.dta    3   28454         14   \n",
       "126            sharew2_rel9-0-0_gv_health.dta    2   37132         44   \n",
       "207                   sharew8_rel9-0-0_as.dta    8   53695         95   \n",
       "266                   sharew3_rel9-0-0_dq.dta    3   28454        177   \n",
       "1         sharew5_rel9-0-0_gv_imputations.dta    5  330190        279   \n",
       "100                 sharew2_rel9-0-0_cv_r.dta    2   55262         30   \n",
       "101                   sharew2_rel9-0-0_xt.dta    2     726        108   \n",
       "\n",
       "                                               columns  is_normal  is_gv  \n",
       "209  [mergeid, hhid8, country, language_xt, gender_...       True  False  \n",
       "128  [mergeid, hhid2, mergeidp2, coupleid2, country...      False  False  \n",
       "278  [mergeid, hhid9, country, language_xt, gender_...       True  False  \n",
       "264  [mergeid, hhid3, mergeidp3, coupleid3, country...       True   True  \n",
       "126  [mergeid, hhid2, mergeidp2, coupleid2, country...       True   True  \n",
       "207  [mergeid, hhid8, mergeidp8, coupleid8, country...       True  False  \n",
       "266  [mergeid, hhid3, mergeidp3, coupleid3, country...       True  False  \n",
       "1    [mergeid, hhid5, mergeidp5, coupleid5, country...       True   True  \n",
       "100  [mergeid, hhid2, mergeidp2, coupleid2, country...      False  False  \n",
       "101  [mergeid, hhid2, country, language_xt, gender_...       True  False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the dataframe to a csv file\n",
    "df.to_csv('data_info.csv', index=False)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get column names for each wave\n",
    "\n",
    "We store the column names for each wave in `.csv` files, so we can use them later. The aim is that we don't have to open all files just for the columns names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave 1 has 483 unique columns.\n",
      "Wave 2 has 452 unique columns.\n",
      "Wave 3 has 151 unique columns.\n",
      "Wave 4 has 89 unique columns.\n",
      "Wave 5 has 99 unique columns.\n",
      "Wave 6 has 2810 unique columns.\n",
      "Wave 7 has 239 unique columns.\n",
      "Wave 8 has 199 unique columns.\n",
      "Wave 9 has 853 unique columns.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data_info.csv')\n",
    "df = df[df['is_gv'] == True] # only keep the general variables files)\n",
    "\n",
    "# iterate through the waves\n",
    "for wave in range(1,9+1):\n",
    "    columns_properties = pd.DataFrame()\n",
    "    subset = df[df['wave'] == wave]\n",
    "\n",
    "    # iterate through the files in the wave\n",
    "    for columns,filename in zip(subset['columns'],subset['file_name']):\n",
    "        \n",
    "        # get all columns and the file they belong to\n",
    "        row = list(columns[1:-1].replace(\"'\", \"\").split(', '))\n",
    "        filenames = [filename]*len(row)\n",
    "\n",
    "        # add the columns and the file to the dataframe\n",
    "        temp = pd.DataFrame({'column': row, 'file_name': filenames})\n",
    "        columns_properties = pd.concat([columns_properties, temp])\n",
    "\n",
    "    columns_properties.drop_duplicates(inplace=True)\n",
    "    print(f'Wave {wave} has {len(columns)} unique columns.')\n",
    "\n",
    "    # remove all column names dupplicate (keep first occurence)\n",
    "    columns_properties.drop_duplicates(subset='column', keep='first', inplace=True)\n",
    "\n",
    "    # save as csv file\n",
    "    columns_properties.to_csv(f'columns/wave_{wave}_columns.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>language</td>\n",
       "      <td>sharew1_rel9-0-0_gv_weights.dta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     column                        file_name\n",
       "5  language  sharew1_rel9-0-0_gv_weights.dta"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('columns/wave_1_columns.csv')\n",
    "test[test['column']=='language']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
